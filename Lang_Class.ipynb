{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "My aim here is to build a language classifier for EU languages.\n",
    "\n",
    "Proposed Approach:\n",
    "1. Inspect test set\n",
    "1. Create dataset for training / validation\n",
    "1. Train / valid split\n",
    "1. Numericalize\n",
    "1. Create embeddings\n",
    "1. Build language classification model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from os import path\n",
    "#from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
    "#platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
    "\n",
    "#accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
    "\n",
    "#!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.0-{platform}-linux_x86_64.whl torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count(x): return len(x.split())\n",
    "def sentence_count(x): return len(x.split('<eos>')) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = Path('data')\n",
    "TRAIN = PATH/'train_sampl'\n",
    "TEST_FN = PATH/'test'\n",
    "PATH_TMP = PATH/'tmp'\n",
    "\n",
    "MIN_FREQ = 20\n",
    "\n",
    "BS = 64\n",
    "\n",
    "EMB_SZ = 1000\n",
    "\n",
    "LANGS = list(map(lambda x: x.name, list(TRAIN.iterdir())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TMP.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clarify Goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(TEST_FN, sep = '\\t', lineterminator='\\n', header=None)\n",
    "test.rename({0:'label', 1:'text'}, axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bg</td>\n",
       "      <td>Европа 2020 не трябва да стартира нов конкурен...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bg</td>\n",
       "      <td>(CS) Най-голямата несправедливост на сегашната...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bg</td>\n",
       "      <td>(DE) Г-жо председател, г-н член на Комисията, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bg</td>\n",
       "      <td>(DE) Г-н председател, бих искал да започна с к...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bg</td>\n",
       "      <td>(DE) Г-н председател, въпросът за правата на ч...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text\n",
       "0    bg  Европа 2020 не трябва да стартира нов конкурен...\n",
       "1    bg  (CS) Най-голямата несправедливост на сегашната...\n",
       "2    bg  (DE) Г-жо председател, г-н член на Комисията, ...\n",
       "3    bg  (DE) Г-н председател, бих искал да започна с к...\n",
       "4    bg  (DE) Г-н председател, въпросът за правата на ч..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['text'] = test['text'].apply(utils.preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<punct> BG <punct> Thank you <punct> Mr President <eos>'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[test['label']=='en'].iloc[0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<punct> BG <punct> Herr Kommissar <eos> Das Dokument <punct> das vom Europäischen Parlament angenommen werden soll <punct> ist in der Tat sehr wichtig <eos>'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[test['label']=='de'].iloc[0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>20828.000000</td>\n",
       "      <td>20828.000000</td>\n",
       "      <td>20828.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.113837</td>\n",
       "      <td>26.071922</td>\n",
       "      <td>175.205541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.851456</td>\n",
       "      <td>25.298060</td>\n",
       "      <td>167.062078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>22.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>99.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>154.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>224.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>77.000000</td>\n",
       "      <td>2184.000000</td>\n",
       "      <td>14069.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentence_count    word_count           len\n",
       "count    20828.000000  20828.000000  20828.000000\n",
       "mean         1.113837     26.071922    175.205541\n",
       "std          0.851456     25.298060    167.062078\n",
       "min          1.000000      3.000000     22.000000\n",
       "25%          1.000000     15.000000     99.000000\n",
       "50%          1.000000     23.000000    154.000000\n",
       "75%          1.000000     33.000000    224.000000\n",
       "max         77.000000   2184.000000  14069.000000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['text'].apply([sentence_count, word_count, len]).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<CHAPTER ID=\"012\">\\nApproval of the minutes of the previous sitting: see Minutes\\n<CHAPTER ID=\"011\">\\nClimate and energy package and maritime transport package (signature of acts) \\n<SPEAKER ID=\"333\" NAME'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exampl = utils.concat_docs('en', TRAIN)\n",
    "exampl[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Approval of the minutes of the previous sitting <punct> see Minutes <eos>',\n",
       " 'Climate and energy package and maritime transport package <punct> signature of acts <punct> <eos>']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exampl = utils.txt2list(exampl)\n",
    "exampl[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " fi  et  it  lt  pt  lv  nl  pl  bg  en  sk  fr  da  hu  cs  sl  es  el  ro  de  sv "
     ]
    }
   ],
   "source": [
    "dfs = []\n",
    "for lang in LANGS:\n",
    "    print(' '+lang+' ', end = \"\")\n",
    "    txt = utils.concat_random_sent(utils.txt2list(utils.concat_docs(lang, TRAIN)))\n",
    "    temp_df = pd.DataFrame({'text':txt})\n",
    "    temp_df['label'] = lang\n",
    "    dfs.append(temp_df)\n",
    "df = pd.concat(dfs)[['label', 'text']]\n",
    "df.reset_index(inplace=True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fi</td>\n",
       "      <td>Suulliset kysymykset ja kirjalliset kannanotot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fi</td>\n",
       "      <td>pöytäkirja &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fi</td>\n",
       "      <td>Ihmisoikeudet sekä sosiaali &lt;punct&gt; ja ympäris...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fi</td>\n",
       "      <td>&lt;punct&gt; DE &lt;punct&gt; Esityslistalla on seuraavan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fi</td>\n",
       "      <td>Tokia Saïfin kansainvälisen kaupan valiokunnan...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text\n",
       "0    fi  Suulliset kysymykset ja kirjalliset kannanotot...\n",
       "1    fi                                   pöytäkirja <eos>\n",
       "2    fi  Ihmisoikeudet sekä sosiaali <punct> ja ympäris...\n",
       "3    fi  <punct> DE <punct> Esityslistalla on seuraavan...\n",
       "4    fi  Tokia Saïfin kansainvälisen kaupan valiokunnan..."
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>298576.000000</td>\n",
       "      <td>298576.000000</td>\n",
       "      <td>298576.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.020132</td>\n",
       "      <td>25.547683</td>\n",
       "      <td>168.444379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.142041</td>\n",
       "      <td>16.804757</td>\n",
       "      <td>111.040280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>16.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>90.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>147.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>220.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>694.000000</td>\n",
       "      <td>4954.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentence_count     word_count            len\n",
       "count   298576.000000  298576.000000  298576.000000\n",
       "mean         1.020132      25.547683     168.444379\n",
       "std          0.142041      16.804757     111.040280\n",
       "min          1.000000       2.000000      16.000000\n",
       "25%          1.000000      14.000000      90.000000\n",
       "50%          1.000000      22.000000     147.000000\n",
       "75%          1.000000      33.000000     220.000000\n",
       "max          3.000000     694.000000    4954.000000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].apply([sentence_count, word_count, len]).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "298576"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(np.array(df['text']), np.array(df['label']), \n",
    "                                                  test_size=0.01, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['sv', 'it', 'sv', ..., 'en', 'fr', 'en'], dtype=object)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numericalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<punct>', 604149),\n",
       " ('<eos>', 301556),\n",
       " ('de', 135517),\n",
       " ('a', 71678),\n",
       " ('<num>', 64662),\n",
       " ('the', 58204),\n",
       " ('en', 49973),\n",
       " ('la', 49668),\n",
       " ('in', 43814),\n",
       " ('que', 41364)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = Counter()\n",
    "for row in X_train: words.update(row.split())\n",
    "words.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = {k:v for k, v in words.items() if v >= MIN_FREQ}\n",
    "words = sorted(words, key=words.get, reverse=True)\n",
    "words = ['<unk>','<pad>'] + words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33485"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(words)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = defaultdict(lambda: 0, {o:i for i,o in enumerate(words)})\n",
    "idx2word = defaultdict(lambda: '<unk>', {i:o for i,o in enumerate(words)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[273, 2671, 6349, 8803, 53, 209, 1387, 2, 64, 1387, 0, 48, 0, 0, 3]\n"
     ]
    }
   ],
   "source": [
    "print([word2idx[w] for w in X_train[0].split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = utils.numericalize(X_train, word2idx)\n",
    "X_val = utils.numericalize(X_val, word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Den borde försöka dra med sig medlemsstaterna <punct> för medlemsstaterna <unk> på <unk> <unk> <eos>',\n",
       " 'Infatti l <punct> Assemblea li <unk> ad istituire sistemi pensionistici più adeguati per assicurare un <unk> di vita <unk> per tutti <punct> <unk> un <punct> attenzione particolare alle categorie più <unk> della società <eos>']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.de_numericalize(X_train[:2], idx2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, n = X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_freq = Counter()\n",
    "for row in X_train: idx_freq.update(row)\n",
    "idx_freq = np.array([idx_freq[i] for i in range(len(idx_freq))]).astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 0.9637772558403621),\n",
       " ('in', 0.9584221102140329),\n",
       " ('of', 0.9500464673857614),\n",
       " ('president', 0.5517612991263541),\n",
       " ('approval', 0.0),\n",
       " ('origin', 0.0)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(x, utils.subsamp_disc_prob(idx_freq)[word2idx[x]]) for x in \n",
    " ['the', 'in', 'of', 'president', 'approval', 'origin']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_skip, y_skip = utils.skipgram_data(X_train, idx_freq)\n",
    "skip_dl = DataLoader(TensorDataset(X_skip, y_skip), batch_size=BS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class skip_model(nn.Module):\n",
    "    def __init__(self, emb_sz = EMB_SZ, vocab_size = vocab_size):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb_sz)\n",
    "        self.target_emb = nn.Embedding(vocab_size, emb_sz)\n",
    "        self.emb.weight.data.uniform_(-0.05, 0.05)\n",
    "        self.target_emb.weight.data.uniform_(-0.05, 0.05)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        context, target = x[:, 0], x[:, 1]\n",
    "        context, target = self.emb(context), self.target_emb(target)\n",
    "        res = (context * target).sum(1)\n",
    "        res = torch.sigmoid(res)\n",
    "        return res.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = skip_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.BCELoss()  # Binary cross entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_batch(xb, yb, model, loss_func, opt):\n",
    "    '''https://github.com/fastai/fastai_v1/blob/master/dev_nb/001a_nn_basics.ipynb'''\n",
    "    # Note: changed this by adding yb.view(-1) to match dimensions\n",
    "\n",
    "    loss = loss_func(model(xb), yb.view(-1))\n",
    "\n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    return loss.item(), len(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner(object):\n",
    "    \n",
    "    def __init__(self, model, loss_func, train_dl = None, valid_dl = None):\n",
    "        self.model = model\n",
    "        self.loss_func = loss_func\n",
    "        self.train_dl = train_dl\n",
    "        self.valid_dl = valid_dl\n",
    "        self.losses = []\n",
    "    \n",
    "    def lr_find(self, start = 1e-5, return_losses = False, exp_smooth = True):\n",
    "        lr = start; lrs = []; losses = []\n",
    "        self.model.train()\n",
    "        i = 0\n",
    "        for xb,yb in tqdm(self.train_dl, position=0):\n",
    "            opt = optim.SGD(self.model.parameters(), lr=lr, momentum=0.9)\n",
    "            l = self.loss_func(self.model(xb), yb.view(-1))\n",
    "            l.backward(); opt.step(); opt.zero_grad()\n",
    "            if exp_smooth or i ==0:\n",
    "                loss = l.detach().numpy()\n",
    "            else:\n",
    "                loss = 0.9*loss + l.detach().numpy()            \n",
    "            lrs.append(lr), losses.append(loss)\n",
    "            if i > 10 and loss > 5*np.mean(losses[:i]):\n",
    "                break\n",
    "            if lr > 1000:\n",
    "                break\n",
    "            lr = lr*1.05\n",
    "            i += 1        \n",
    "        f, ax = plt.subplots(figsize=(5, 5))\n",
    "        ax.set(yscale = 'log', xscale = 'log')\n",
    "        ax = plt.plot(lrs, losses)\n",
    "        self.losses = losses\n",
    "        if return_losses: return (lrs, losses)\n",
    "        \n",
    "    def plot_loss(self):\n",
    "        f, ax = plt.subplots(figsize=(5, 5))\n",
    "        ax.set(yscale = 'log')\n",
    "        ax = plt.plot(self.losses)\n",
    "            \n",
    "    def fit(self, lr, epochs, callOn_epoch_start = None):\n",
    "        \n",
    "        self.opt = optim.SGD(self.model.parameters(), lr=lr, momentum=0.9)\n",
    "        loss_list = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            if callOn_epoch_start:\n",
    "                callOn_epoch_start()\n",
    "        \n",
    "            # Fit model to training data\n",
    "            self.model.train()\n",
    "            losses, nums = zip(*[loss_batch(xb, yb, self.model, self.loss_func, self.opt) \n",
    "                                 for xb,yb in tqdm(self.train_dl, position=0)])\n",
    "            train_loss = np.sum(np.multiply(losses,nums)) / np.sum(nums)\n",
    "            loss_list = loss_list+list(losses)\n",
    "\n",
    "            # Calculate loss on validation set\n",
    "            if self.valid_dl != None:\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    losses,nums = zip(*[loss_batch(model, loss_func, xb, yb)\n",
    "                                        for xb,yb in valid_dl])\n",
    "                val_loss = np.sum(np.multiply(losses,nums)) / np.sum(nums)\n",
    "            else:\n",
    "                val_loss = 'N/A'\n",
    "                \n",
    "            print(f'Epoch {epoch}. Training loss: {train_loss}. Validation loss: {val_loss}.')\n",
    "        self.losses = loss_list\n",
    "            \n",
    "class skipgram_Learner(Learner):\n",
    "    def update_train(self):\n",
    "        X_skip, y_skip = utils.skipgram_data(X_train, idx_freq)\n",
    "        self.train_dl = DataLoader(TensorDataset(X_skip, y_skip), batch_size=BS)\n",
    "        \n",
    "    def fit(self, lr, epochs): \n",
    "        super().fit(lr, epochs, callOn_epoch_start=self.update_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = skipgram_Learner(model, loss_func, skip_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 9/4065 [00:03<22:09,  3.05it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-d81c6bd29d71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_find\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-50-f5a4b655e822>\u001b[0m in \u001b[0;36mlr_find\u001b[0;34m(self, start, return_losses, exp_smooth)\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mexp_smooth\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 10/4098 [00:02<17:48,  3.83it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-0333c9c09486>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-50-f5a4b655e822>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, lr, epochs)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallOn_epoch_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-50-f5a4b655e822>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, lr, epochs, callOn_epoch_start)\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             losses, nums = zip(*[loss_batch(xb, yb, self.model, self.loss_func, self.opt) \n\u001b[0;32m---> 53\u001b[0;31m                                  for xb,yb in tqdm(self.train_dl, position=0)])\n\u001b[0m\u001b[1;32m     54\u001b[0m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnums\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnums\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mloss_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_list\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-50-f5a4b655e822>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             losses, nums = zip(*[loss_batch(xb, yb, self.model, self.loss_func, self.opt) \n\u001b[0;32m---> 53\u001b[0;31m                                  for xb,yb in tqdm(self.train_dl, position=0)])\n\u001b[0m\u001b[1;32m     54\u001b[0m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnums\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnums\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mloss_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_list\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-a7aebcb7d792>\u001b[0m in \u001b[0;36mloss_batch\u001b[0;34m(xb, yb, model, loss_func, opt)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mopt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     99\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'momentum_buffer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m                         \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdampening\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mnesterov\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                         \u001b[0md_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md_p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learn.fit(0.1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/krisztian/anaconda3/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type skip_model. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(sgFitter.model, PATH_TMP/'embeddings.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs = sgFitter.model.emb.weight.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_dist(u, v): return np.dot(u, v) / np.sqrt(np.sum(u**2)*np.sum(v**2))\n",
    "def emb_pair_dist(a, b, c, d):\n",
    "    return cos_dist(embs[word2idx[a]] - embs[word2idx[b]],\n",
    "                   embs[word2idx[c]] - embs[word2idx[d]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.032636125"
      ]
     },
     "execution_count": 526,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_pair_dist('man', 'woman', 'he', 'she')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0030198644"
      ]
     },
     "execution_count": 527,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_pair_dist('good', 'better', 'bad', 'worse')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
