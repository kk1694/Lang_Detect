{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "My aim here is to build a language classifier for EU languages.\n",
    "\n",
    "Proposed Approach:\n",
    "1. Inspect test set\n",
    "1. Create dataset for training / validation\n",
    "1. Train / valid split\n",
    "1. Numericalize\n",
    "1. Create embeddings\n",
    "1. Build language classification model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from os import path\n",
    "#from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
    "#platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
    "\n",
    "#accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
    "\n",
    "#!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.0-{platform}-linux_x86_64.whl torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import dill\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = Path('data')  # Directory for all data and temporary files\n",
    "TRAIN = PATH/'train'  # Directory for training text\n",
    "TEST_FN = PATH/'test'  # Filename for test text\n",
    "PATH_TMP = PATH/'tmp'  # Temporary directory to save progress\n",
    "\n",
    "MIN_FREQ = 30  # We'll replace words with lower frequency with unknown\n",
    "\n",
    "BS = 64  # Batch size for our RNN\n",
    "SKIP_BS = 512  # Batch size for skip-gram. Can be high.\n",
    "\n",
    "EMB_SZ = 200  # Dimension of word embeddings\n",
    "HIDDEN_SZ = 100\n",
    "\n",
    "# List of language\n",
    "LANGS = list(map(lambda x: x.name, list(TRAIN.iterdir())))\n",
    "\n",
    "assert torch.cuda.is_available()  # Notebook is written for GPU computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TMP.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clarify Goal\n",
    "\n",
    "Let's first have a look at the test set we are trying to predict. It looks like a simple text classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(TEST_FN, sep = '\\t', lineterminator='\\n', header=None)\n",
    "test.rename({0:'label', 1:'text'}, axis = 1, inplace=True)\n",
    "test[test['label'] == 'en'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before going any further, let's apply some preprocessing. In particular, I apply the following steps:\n",
    "1. Remove uninformative meta-comments, such as who is speaking).\n",
    "1. Replace numbers with a generic <num> token. After all, the specific number shouldn't affect the classification results.\n",
    "1. Create a special end-of-sentence (<eos>) token.\n",
    "1. Replace all punctuation with a special <punc> token. \n",
    "1. Collapse adjecent white space. In other words, '   ' becomes ' '."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['text'] = test['text'].apply(utils.preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check a random English and German sentence after pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test[test['label']=='en'].iloc[0][\"text\"])\n",
    "print('---')\n",
    "print(test[test['label']=='de'].iloc[0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target for our classification model has the following characteristics:\n",
    "1. The vast majority of examples are a single sentence.\n",
    "1. Most of the time, we have a decent number of words (15-33) to predict a language.\n",
    "1. However, we can have as little as 3 words. This might pose a challenge if those words are not language-specific."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def word_count(x): return len(x.split())\n",
    "def sentence_count(x): return len(x.split('<eos>')) - 1\n",
    "test['text'].apply([sentence_count, word_count, len]).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exampl = utils.concat_docs('en', TRAIN)\n",
    "exampl[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exampl = utils.txt2list(exampl[:1000])\n",
    "exampl[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for lang in LANGS:\n",
    "    print(' '+lang+' ', end = \"\")\n",
    "    txt = utils.concat_random_sent(utils.txt2list(utils.concat_docs(lang, TRAIN)))\n",
    "    temp_df = pd.DataFrame({'text':txt})\n",
    "    temp_df['label'] = lang\n",
    "    dfs.append(temp_df)\n",
    "df = pd.concat(dfs)[['label', 'text']]\n",
    "df.reset_index(inplace=True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'].apply([sentence_count, word_count, len]).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(dfs, temp_df, txt, exampl)\n",
    "dill.dump(df, open(PATH_TMP/'df.pickle', mode = 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = dill.load(open(PATH_TMP/'df.pickle', mode = 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(np.array(df['text']), np.array(df['label']), \n",
    "                                                  test_size=0.01, random_state=42)\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numericalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = Counter()\n",
    "for row in tqdm(X_train, position=0, leave=False): words.update(row.split())\n",
    "words.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = {k:v for k, v in tqdm(words.items(), leave = False) if v >= MIN_FREQ}\n",
    "words = sorted(words, key=words.get, reverse=True)\n",
    "words = ['<unk>','<pad>'] + words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(words)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = defaultdict(lambda: 0, {o:i for i,o in enumerate(words)})\n",
    "idx2word = defaultdict(lambda: '<unk>', {i:o for i,o in enumerate(words)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([word2idx[w] for w in X_train[0].split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = utils.numericalize(X_train, word2idx)\n",
    "X_val = utils.numericalize(X_val, word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.de_numericalize(X_train[:2], idx2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang2idx = defaultdict(lambda: 0, {o:i for i,o in enumerate(LANGS)})\n",
    "idx2lang = defaultdict(lambda: '<unk>', {i:o for i,o in enumerate(LANGS)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array([lang2idx[x] for x in y_train])\n",
    "y_val = np.array([lang2idx[x] for x in y_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PATH_TMP/'numericalized.pickle', mode = 'wb') as f:\n",
    "    dill.dump([words, vocab_size, word2idx, idx2word, X_train, X_val, y_train, y_val], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(PATH_TMP/'numericalized.pickle', mode = 'rb') as f:\n",
    "#    (words, vocab_size, word2idx, idx2word, X_train, X_val, y_train, y_val) = dill.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, n = X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_freq = Counter()\n",
    "for row in tqdm(X_train, leave = False): idx_freq.update(row)\n",
    "idx_freq = np.array([idx_freq[i] for i in tqdm(range(vocab_size), leave = False)]).astype(np.int32)\n",
    "idx_freq = np.maximum(idx_freq, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(x, utils.subsamp_disc_prob(idx_freq)[word2idx[x]]) for x in \n",
    " ['the', 'in', 'of', 'president', 'approval', 'origin']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_skip, y_skip = utils.skipgram_data(X_train, idx_freq)\n",
    "skip_dl = DataLoader(TensorDataset(X_skip, y_skip), batch_size=SKIP_BS, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class skip_model(nn.Module):\n",
    "    def __init__(self, emb_sz = EMB_SZ, vocab_size = vocab_size):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb_sz)\n",
    "        self.target_emb = nn.Embedding(vocab_size, emb_sz)\n",
    "        self.emb.weight.data.uniform_(-0.05, 0.05)\n",
    "        self.target_emb.weight.data.uniform_(-0.05, 0.05)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        context, target = x[:, 0], x[:, 1]\n",
    "        context, target = self.emb(context), self.target_emb(target)\n",
    "        res = (context * target).sum(1)\n",
    "        res = torch.sigmoid(res)\n",
    "        return res.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = skip_model().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.BCELoss().cuda()  # Binary cross entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_batch(xb, yb, model, loss_func, opt):\n",
    "    '''https://github.com/fastai/fastai_v1/blob/master/dev_nb/001a_nn_basics.ipynb'''\n",
    "    # Note: changed this by adding yb.view(-1) to match dimensions\n",
    "\n",
    "    loss = loss_func(model(xb.cuda()), yb.cuda().view(-1))\n",
    "\n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    return loss.item(), len(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner(object):\n",
    "    \n",
    "    def __init__(self, model, loss_func, train_dl = None, valid_dl = None):\n",
    "        self.model = model\n",
    "        self.loss_func = loss_func\n",
    "        self.train_dl = train_dl\n",
    "        self.valid_dl = valid_dl\n",
    "        self.losses = []\n",
    "    \n",
    "    def lr_find(self, start = 1e-6, end = 1e1,\n",
    "                exp_smooth = True,\n",
    "               skip_first = 10, skip_last = 10):\n",
    "        lr = start; lrs = []; losses = []\n",
    "        self.model.train()\n",
    "        i = 0\n",
    "        for xb,yb in self.train_dl:\n",
    "            opt = optim.Adam(self.model.parameters(), lr=lr)\n",
    "            l, _ = loss_batch(xb, yb, self.model, self.loss_func, opt)\n",
    "            if (not exp_smooth) or i ==0:\n",
    "                loss = l\n",
    "            else:\n",
    "                loss = 0.9*loss + 0.1*l\n",
    "            if (i+1)%100 == 0:\n",
    "                print(f'iteration {i}, lr = {lr}, loss = {loss}')\n",
    "            lrs.append(lr), losses.append(loss)\n",
    "            if (lr > end) or (i > 10 and loss > 3*np.mean(losses[:i])):\n",
    "                break\n",
    "            lr *= 1.01; i += 1        \n",
    "        f, ax = plt.subplots(figsize=(5, 5))\n",
    "        ax.set(yscale = 'log', xscale = 'log')\n",
    "        ax = plt.plot(lrs[skip_first:-skip_last], losses[skip_first:-skip_last])\n",
    "        self.losses = losses\n",
    "        \n",
    "        # Re-initialize embeddings\n",
    "        self.model.emb.weight.data.uniform_(-0.05, 0.05)\n",
    "        self.model.target_emb.weight.data.uniform_(-0.05, 0.05)\n",
    "        \n",
    "    def plot_loss(self):\n",
    "        f, ax = plt.subplots(figsize=(5, 5))\n",
    "        ax.set(yscale = 'log')\n",
    "        ax = plt.plot(self.losses)\n",
    "            \n",
    "    def fit(self, lr, epochs, callOn_epoch_start = None):\n",
    "        \n",
    "        opt = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        loss_list = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            if callOn_epoch_start:\n",
    "                callOn_epoch_start()\n",
    "        \n",
    "            # Fit model to training data\n",
    "            self.model.train()\n",
    "            losses, nums = zip(*[loss_batch(xb, yb, self.model, self.loss_func, opt) \n",
    "                                 for xb,yb in tqdm(self.train_dl, position=0, leave = False)])\n",
    "            train_loss = np.sum(np.multiply(losses,nums)) / np.sum(nums)\n",
    "            loss_list = loss_list+list(losses)\n",
    "\n",
    "            # Calculate loss on validation set\n",
    "            if self.valid_dl != None:\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    losses,nums = zip(*[loss_batch(model, loss_func, xb, yb)\n",
    "                                        for xb,yb in valid_dl])\n",
    "                val_loss = np.sum(np.multiply(losses,nums)) / np.sum(nums)\n",
    "                print(f'Epoch {epoch}. Training loss: {train_loss}. Validation loss: {val_loss}.')\n",
    "            else:\n",
    "                print(f'Epoch {epoch}. Training loss: {train_loss}.')\n",
    "                \n",
    "            \n",
    "        self.losses = loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class skipgram_Learner(Learner):\n",
    "    def update_train(self):\n",
    "        X_skip, y_skip = utils.skipgram_data(X_train, idx_freq)\n",
    "        self.train_dl = DataLoader(TensorDataset(X_skip, y_skip), batch_size=SKIP_BS, shuffle = True)\n",
    "        \n",
    "    def fit(self, lr, epochs): \n",
    "        super().fit(lr, epochs, callOn_epoch_start=self.update_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = skipgram_Learner(model, loss_func, skip_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn.fit(lr, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(learn.model, PATH_TMP/'embeddings0.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs = learn.model.emb.weight.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_dist(u, v): return np.dot(u, v) / np.sqrt(np.sum(u**2)*np.sum(v**2))\n",
    "def emb_pair_dist(a, b, c, d):\n",
    "    return cos_dist(embs[word2idx[a]] - embs[word2idx[b]],\n",
    "                   embs[word2idx[c]] - embs[word2idx[d]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_pair_dist('man', 'woman', 'he', 'she')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_pair_dist('good', 'better', 'bad', 'worse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.from_numpy(X_train).type(torch.int64)\n",
    "y_train = torch.from_numpy(y_train).type(torch.int64)\n",
    "X_val = torch.from_numpy(X_val).type(torch.int64)\n",
    "y_val = torch.from_numpy(y_val).type(torch.int64)\n",
    "\n",
    "train_dl = DataLoader(TensorDataset(X_train, y_train), batch_size=BS, shuffle = True)\n",
    "valid_dl = DataLoader(TensorDataset(X_val, y_val), batch_size=BS, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lang_Detect(nn.Module):\n",
    "    def __init__(self, emb_sz = EMB_SZ, vocab_size = vocab_size,\n",
    "                hidden_sz = HIDDEN_SZ, out_sz = len(LANGS)):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb_sz)\n",
    "        self.emb_drop = nn.Dropout(0.25)\n",
    "        self.emb.weight.data.uniform_(-0.05, 0.05)\n",
    "        self.gru = nn.GRU(emb_sz, hidden_sz)\n",
    "        self.drop = nn.Dropout(0.25)\n",
    "        self.lout = nn.Linear(hidden_sz, out_sz)\n",
    "        self.hidden_sz = hidden_sz\n",
    "                \n",
    "    def forward(self, seq): \n",
    "        bs, _ = seq.shape\n",
    "        h =  torch.zeros(1, bs, self.hidden_sz).cuda()\n",
    "        embedded = self.emb(seq).transpose(0, 1)\n",
    "        outputs, _ = self.gru(self.emb_drop(embedded), h)\n",
    "        output = self.lout(self.drop(outputs[-1]))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Lang_Detect().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_batch(xb, yb, model, loss_func, opt):\n",
    "    '''https://github.com/fastai/fastai_v1/blob/master/dev_nb/001a_nn_basics.ipynb'''\n",
    "    # Note: changed this by adding yb.view(-1) to match dimensions\n",
    "\n",
    "    loss = loss_func(model(xb.cuda()), yb.cuda())\n",
    "\n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    return loss.item(), len(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner(object):\n",
    "    \n",
    "    def __init__(self, model, loss_func, train_dl = None, valid_dl = None):\n",
    "        self.model = model\n",
    "        self.loss_func = loss_func\n",
    "        self.train_dl = train_dl\n",
    "        self.valid_dl = valid_dl\n",
    "        self.losses = []\n",
    "    \n",
    "    def lr_find(self, start = 1e-6, end = 1e1, exp_smooth_param = 0.9):\n",
    "        \n",
    "        self.model.train()\n",
    "        lr = start; lrs = []; losses = []; i = 0\n",
    "        for xb,yb in tqdm(self.train_dl, leave = False,\n",
    "                         position = 0):\n",
    "            opt = optim.Adam(self.model.parameters(), lr=lr)\n",
    "            loss, _ = loss_batch(xb, yb, self.model, self.loss_func, opt)\n",
    "            lrs.append(lr), losses.append(loss)\n",
    "            if (lr > end) or (i > 10 and loss > 3*np.mean(losses[:i])):\n",
    "                break\n",
    "            lr *= 1.01; i += 1        \n",
    "        self.losses = losses\n",
    "        self.plot_loss(x = lrs, xlog=True, exp_smooth_param = exp_smooth_param)\n",
    "        \n",
    "    def plot_loss(self, x = None, xlog = False, exp_smooth_param = 0.95,\n",
    "                 skip_edges = False):\n",
    "        y_smooth = utils.exp_smooth(np.array(self.losses), exp_smooth_param)\n",
    "        if skip_edges:\n",
    "            y_smooth = y_smooth[10:-10]\n",
    "        f, ax = plt.subplots(figsize=(5, 5))\n",
    "        if xlog:\n",
    "            ax.set(yscale = 'log', xscale = 'log')\n",
    "        else:\n",
    "            ax.set(yscale = 'log')\n",
    "        if x is not None:\n",
    "            if skip_edges:\n",
    "                x = x[10:-10]\n",
    "            ax = plt.plot(x, y_smooth)\n",
    "        else:\n",
    "            ax = plt.plot(y_smooth)     \n",
    "            \n",
    "    def fit(self, lr, epochs, callOn_epoch_start = None):\n",
    "        \n",
    "        opt = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        loss_list = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            if callOn_epoch_start:\n",
    "                callOn_epoch_start()\n",
    "        \n",
    "            # Fit model to training data\n",
    "            self.model.train()\n",
    "            losses, nums = zip(*[loss_batch(xb, yb, self.model, self.loss_func, opt) \n",
    "                                 for xb,yb in tqdm(self.train_dl, leave = False,\n",
    "                                                  position = 0)])\n",
    "            train_loss = np.sum(np.multiply(losses,nums)) / np.sum(nums)\n",
    "            loss_list = loss_list+list(losses)\n",
    "\n",
    "            # Calculate loss on validation set\n",
    "            if self.valid_dl != None:\n",
    "                self.model.eval()\n",
    "                with torch.no_grad():\n",
    "                    losses,nums = zip(*[loss_batch(model, loss_func, xb, yb)\n",
    "                                        for xb,yb in tqdm(valid_dl, leave = False,\n",
    "                                                         position = 0)])\n",
    "                val_loss = np.sum(np.multiply(losses,nums)) / np.sum(nums)\n",
    "                print(f'Epoch {epoch}. Training loss: {train_loss}. Validation loss: {val_loss}.')\n",
    "            else:\n",
    "                print(f'Epoch {epoch}. Training loss: {train_loss}.')\n",
    "                \n",
    "        self.losses = loss_list\n",
    "        \n",
    "    def predict(self, dl):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            res = [self.model(xb.cuda()).detach().cpu().numpy().argmax(axis = -1) for \n",
    "                   xb, _ in tqdm(dl, leave = False, position = 0)]\n",
    "        return np.concatenate(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(model, loss_func, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 3e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(lr, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = learn.predict(valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(pred, y):\n",
    "    m = y.shape[0]\n",
    "    assert pred.shape == (m,)\n",
    "    return np.sum(pred == y) / m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(preds, y_val.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1/21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
