{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "XXX UPDATE FROM README!.\n",
    "\n",
    "The model that achieves near-perfect (99.9%+) test set accuracy. While fitting the model to the whole data takes a significant amount of computation time (~2 hours), the same model fitted to a 1% subsample of the training data also achieves very good results (XXXX).\n",
    "\n",
    "The table below lists the model results based on sample size. \n",
    "\n",
    "\n",
    "| Dataset           | Model              | Accuracy           | Training Time      | Inference Time     | Vocabulary size    | Link              |\n",
    "| ------------------ | ------------------ | ------------------ | ------------------ | ------------------ | ------------------ | ------------------ |\n",
    "| Full Dataset  | Word Level      | 99.97%             | 136 min      |           | 403619         | [here]() |\n",
    "| 10% Sample    | Word Level      | XXXX               | XXX min      |           | XXX            | [here]() |\n",
    "| 1% Sample     | Word Level      | XXXX               | XXX min      |           | XXX            | [here]() |\n",
    "| 1% Sample     | Character Level | XXXX               | XXX min      |           | XXX            | [here]() |\n",
    "\n",
    "Training time involves all preprocessing and model fitting (but not download time). Inference time involves predicting the ~20k sentences of the test set. The runs are done on a google cloud virtual machine with P100 GPU (the notebook contains full specification).\n",
    "\n",
    "## Modelling Challanges\n",
    "\n",
    "On the face of it, the project seems like a simple sequence classification task. Unless the test set contains text whose languages is ambiguous (the same text could pass as, say both Czech and Slovak) or contains extremely rare words, we would expect a good classifier to achieve near-perfect accuracy.\n",
    "\n",
    "While near-perfect accuracy is indeed possible, there are two possible challenges that need to be overcome.\n",
    "\n",
    "First, we can't rely on language specific information (after all, that's what we are trying to predict). This makes many standard NLP preprocessing steps unavailable. For example, we can't assign {'go', 'goes', 'going'} to the same word token. Even lower-casing is debatable, as some languages (for example German) capitalize words differently, a pattern that can be exploited to predict a language (German capitalizes all nouns).\n",
    "\n",
    "Second, I chose not to use any pretrained models, or transfer learning. The challenge description wasn't clear if doing so is allowed or not; to err on the safe side, I only rely on the supplied training set. Normally, pretrained models and transfer learning is a great resource -- after all, the Internet has practically unlimited text data one can pretrain a model on.\n",
    "\n",
    "## Character or Word Level?\n",
    "\n",
    "Should we build a model based on sequences of characters, or sequences of words? \n",
    "\n",
    "If our test set were to contain many words not appearing in the training set, a character-level model would be appropriate. After all, a word-level model can't generalize outside the words it has seen. However, our training corpus is ~5 GB large, so we would expect it to contain the vast majority of common words in each language. \n",
    "\n",
    "Performance side, a character level model benefits from lower memory usage, but higher run-time. (The embedding space is orders of magnitude smaller, but we have to take many more steps for prediction).\n",
    "\n",
    "I will focus on word-level modeling, but also provide a [short notebook]() on how a character-level model could be implemented (it will achieve somewhat lower accuracy). The steps for both models are almost the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Dependencies\n",
    "\n",
    "I put some uninteresting function definitions into the utils.py file. The text should make it clear what each function accomplishes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm import tqdm\n",
    "import dill  # Better version of pickle, able to save objects with lambda expressions\n",
    "import copy  # Used for making a deep copy of a model\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "start = time.time()\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings\n",
    "\n",
    "I will discuss the reasoning behind hyper-parameter choices in the sections they become relevant. \n",
    "\n",
    "One important note: the notebook is written for GPU computations and will not work on a machine with CPU only (fitting the model on a cpu would take way too long anyway)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = Path('data')  # Directory for all data and temporary files\n",
    "TRAIN = PATH/'train_sampl_10pct'  # Directory for training text\n",
    "TEST_FN = PATH/'test'  # Filename for test text\n",
    "PATH_TMP = PATH/'tmp'  # Temporary directory to save progress\n",
    "\n",
    "WORD_LEVEL = True  # If false, we apply character-level modelling\n",
    "MIN_FREQ = 5  # We'll replace words with lower frequency with unknown\n",
    "\n",
    "SEQ_LEN = 32 if WORD_LEVEL else 256  # Length of the sequences passed into our GRU\n",
    "VAL_FRAC = 0.01  # Validation set percentage\n",
    "\n",
    "# For the batch-size, select the largest power of 2 your GPU can comfortably handle.\n",
    "# However, the optimal learning rate (see model training section) will probably change.\n",
    "BS = 512  # Batch size for our RNN\n",
    "\n",
    "EMB_SZ = 50  # Dimension of word embeddings\n",
    "HIDDEN_SZ = 250  # Hidden layer dimension of the GRU\n",
    "EMB_DROP = 0.25  # Dropout applied to embeddings\n",
    "LAYER_DROP = 0.25  # Dropout applied after GRU\n",
    "LR = 3e-3 if WORD_LEVEL else 1e-2  # Learning rate for fitting\n",
    "EPOCHS = 1 if WORD_LEVEL else 2  # Epochs to train \n",
    "EPOCHS_FT = 3 if WORD_LEVEL else 3  # Epochs to fine-tune with very small learning rate (LR / 100)\n",
    "\n",
    "# List of languages\n",
    "LANGS = list(map(lambda x: x.name, list(TRAIN.iterdir())))\n",
    "\n",
    "assert torch.cuda.is_available()  # Notebook is written for GPU computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TMP.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clarify Goal\n",
    "\n",
    "Let's first have a look at the test set we are trying to predict. It looks like a simple text classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4992</th>\n",
       "      <td>en</td>\n",
       "      <td>(BG) Thank you, Mr President.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4993</th>\n",
       "      <td>en</td>\n",
       "      <td>(EL) Madam President, I agree and recognise Tu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4994</th>\n",
       "      <td>en</td>\n",
       "      <td>(FI) Madam President, firstly, I would like to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>en</td>\n",
       "      <td>(FI) Mr President, the Treaty of Lisbon will r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>en</td>\n",
       "      <td>(FR) Madam President, one of the priorities of...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                               text\n",
       "4992    en                      (BG) Thank you, Mr President.\n",
       "4993    en  (EL) Madam President, I agree and recognise Tu...\n",
       "4994    en  (FI) Madam President, firstly, I would like to...\n",
       "4995    en  (FI) Mr President, the Treaty of Lisbon will r...\n",
       "4996    en  (FR) Madam President, one of the priorities of..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv(TEST_FN, sep = '\\t', lineterminator='\\n', header=None)\n",
    "test.rename({0:'label', 1:'text'}, axis = 1, inplace=True)\n",
    "test[test['label'] == 'en'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before going any further, let's apply some preprocessing. In particular, I apply the following steps:\n",
    "1. Remove uninformative meta-comments, such as who is speaking.\n",
    "1. Replace numbers with a generic (*num*) token. After all, the specific number shouldn't affect the classification results.\n",
    "1. Create special end-of-sentence (*eos*) tokens.\n",
    "1. Replace all punctuation with a special (*punc*) token. \n",
    "1. Collapse adjecent white space. In other words, '&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;' becomes '&nbsp;'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_start = time.time()  # We keep track of the total time needed for preprocessing+prediction\n",
    "test['text'] = test['text'].apply(lambda x: utils.preprocess(x, word_level = WORD_LEVEL))\n",
    "test_time = time.time()-tt_start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the first English and German sentences after pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<punct> BG <punct> Thank you <punct> Mr President <eos>\n",
      "---\n",
      "<punct> BG <punct> Herr Kommissar <eos> Das Dokument <punct> das vom Europäischen Parlament angenommen werden soll <punct> ist in der Tat sehr wichtig <eos>\n"
     ]
    }
   ],
   "source": [
    "print(test[test['label']=='en'].iloc[0][\"text\"])\n",
    "print('---')\n",
    "print(test[test['label']=='de'].iloc[0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>20828.000000</td>\n",
       "      <td>20828.000000</td>\n",
       "      <td>20828.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.113837</td>\n",
       "      <td>26.071922</td>\n",
       "      <td>175.205541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.851456</td>\n",
       "      <td>25.298060</td>\n",
       "      <td>167.062078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>22.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>99.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>154.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>224.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>77.000000</td>\n",
       "      <td>2184.000000</td>\n",
       "      <td>14069.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentence_count    word_count           len\n",
       "count    20828.000000  20828.000000  20828.000000\n",
       "mean         1.113837     26.071922    175.205541\n",
       "std          0.851456     25.298060    167.062078\n",
       "min          1.000000      3.000000     22.000000\n",
       "25%          1.000000     15.000000     99.000000\n",
       "50%          1.000000     23.000000    154.000000\n",
       "75%          1.000000     33.000000    224.000000\n",
       "max         77.000000   2184.000000  14069.000000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def word_count(x): return len(x.split())\n",
    "def sentence_count(x): return len(x.split('<eos>')) - 1\n",
    "test['text'].apply([sentence_count, word_count, len]).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target for our classification model has the following characteristics:\n",
    "1. The vast majority of examples are single sentences.\n",
    "1. Most of the time, we have a decent number of words (15-33) to predict a language.\n",
    "1. However, we can have as little as 3 words. This might pose a challenge if those words are not language-specific.\n",
    "1. Content-wise, most sentences seem to be about parliamentary proceedings (my guess: proceedings of the EUP for more recent years)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Training/Validation Data\n",
    "\n",
    "Our training set is of similar content, but in a different format. Specifically, we don't have sentence-level chunks as in our test set. Instead, we have files containing varying number of sentences. So we need to create a dataframe that resembles our test set.\n",
    "\n",
    "First, we take all files for a language and concatenate them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<CHAPTER ID=\"009-16\">\\n16. Institutional balance of the European Union (\\n<P>\\n- Before the vote\\n<SPEAKER ID=\"133\" NAME=\"Jean-Luc Dehaene\" LANGUAGE=\"NL\">\\nMr President, first of all, I would like to make '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# An example with English, we'll do all processing steps for all files below.\n",
    "exampl = utils.concat_docs('en', TRAIN)\n",
    "exampl[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply the same pre-processing as we did to our test set, and turn the whole corpus into a list of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<num> Institutional balance of the European Union <punct> <eos>',\n",
       " '<punct> Before the vote <eos>',\n",
       " 'Mr President <punct> first of all <punct> I would like to make a technical remark <eos>']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example continued.\n",
    "exampl = utils.txt2list(utils.preprocess(exampl[:1000], word_level=WORD_LEVEL))\n",
    "exampl[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one last step we do. The test set had occasionally (although not often) multiple sentences. So we want to have, occasionally, multiple sentences in our training set as well. We can accomplish this by concatenating adjecent sentences together with a small probability (p = 0.02).\n",
    "\n",
    "Let's apply all the above steps to all languages. I will also put everything into a dataframe with an extra column giving the language label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " fi  et  it  lt  pt  lv  nl  pl  bg  en  sk  fr  da  hu  cs  sl  es  el  ro  de  sv "
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fi</td>\n",
       "      <td>&lt;num&gt; Vastuuvapaus &lt;num&gt; &lt;punct&gt; Euroopan unio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fi</td>\n",
       "      <td>Neuvoston toimittamat sopimustekstit &lt;punct&gt; k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fi</td>\n",
       "      <td>pöytäkirja &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fi</td>\n",
       "      <td>Suulliset kysymykset ja kirjalliset kannanotot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fi</td>\n",
       "      <td>pöytäkirja &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text\n",
       "0    fi  <num> Vastuuvapaus <num> <punct> Euroopan unio...\n",
       "1    fi  Neuvoston toimittamat sopimustekstit <punct> k...\n",
       "2    fi                                   pöytäkirja <eos>\n",
       "3    fi  Suulliset kysymykset ja kirjalliset kannanotot...\n",
       "4    fi                                   pöytäkirja <eos>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs = []  # List to store data frames\n",
    "for lang in LANGS:\n",
    "    print(' '+lang+' ', end = \"\")\n",
    "    txt = utils.concat_docs(lang, TRAIN)                # 1. Concatenate all files\n",
    "    txt = utils.preprocess(txt, word_level=WORD_LEVEL)  # 2. Apply preprocessing described in test section\n",
    "    txt = utils.txt2list(txt)                           # 3. Convert to list\n",
    "    txt = utils.concat_random_sent(txt, p = 0.02)       # 4. Concatenate random adjecent sentences\n",
    "    temp_df = pd.DataFrame({'text':txt})                # 5. Convert to dataframe\n",
    "    temp_df['label'] = lang                             # 6. Add label   \n",
    "    dfs.append(temp_df)\n",
    "df = pd.concat(dfs)[['label', 'text']]\n",
    "df.reset_index(inplace=True, drop = True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3.025903e+06</td>\n",
       "      <td>3.025903e+06</td>\n",
       "      <td>3.025903e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.020191e+00</td>\n",
       "      <td>2.502545e+01</td>\n",
       "      <td>1.657363e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.421583e-01</td>\n",
       "      <td>1.658663e+01</td>\n",
       "      <td>1.099864e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>9.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.300000e+01</td>\n",
       "      <td>8.800000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2.200000e+01</td>\n",
       "      <td>1.450000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>3.300000e+01</td>\n",
       "      <td>2.180000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>7.970000e+02</td>\n",
       "      <td>5.052000e+03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentence_count    word_count           len\n",
       "count    3.025903e+06  3.025903e+06  3.025903e+06\n",
       "mean     1.020191e+00  2.502545e+01  1.657363e+02\n",
       "std      1.421583e-01  1.658663e+01  1.099864e+02\n",
       "min      1.000000e+00  2.000000e+00  9.000000e+00\n",
       "25%      1.000000e+00  1.300000e+01  8.800000e+01\n",
       "50%      1.000000e+00  2.200000e+01  1.450000e+02\n",
       "75%      1.000000e+00  3.300000e+01  2.180000e+02\n",
       "max      4.000000e+00  7.970000e+02  5.052000e+03"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].apply([sentence_count, word_count, len]).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our resulting dataframe looks very similar to our test set. \n",
    "\n",
    "One difference: we don't go quite as high on the maximum words and sentences. That difference is not going to matter though, as later on all texts will be truncated at 32 words anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(dfs, temp_df, txt, exampl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dill.dump(df, open(PATH_TMP/'df.pickle', mode = 'wb'))\n",
    "#df = dill.load(open(PATH_TMP/'df.pickle', mode = 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Validation Split\n",
    "\n",
    "Let's split the data for training and validation. No big surprises here.\n",
    "\n",
    "I use 1% of the data as validation. If that seems unusual, note that our dataset contains ~30 million, rows, so our validation set will contain ~300k. I'm only using the validation set to monitor performance and check for over-fitting. 300k examples are enough for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3025903"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['cs', 'bg', 'sl', ..., 'es', 'sv', 'es'], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(np.array(df['text']), np.array(df['label']), \n",
    "                                                  test_size=VAL_FRAC, random_state=42)\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numericalize\n",
    "\n",
    "We need to turn our words into intiger indices. Later we use these indices to look up the embeddings for each word.\n",
    "\n",
    "Let's start by counting the number of times a word appears in our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "text/plain": [
       "[('<punct>', 5958957),\n",
       " ('<eos>', 3056143),\n",
       " ('de', 1493923),\n",
       " ('a', 738400),\n",
       " ('<num>', 604638),\n",
       " ('la', 525494),\n",
       " ('en', 499361),\n",
       " ('que', 469173),\n",
       " ('in', 424132),\n",
       " ('the', 360008)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = Counter()\n",
    "if WORD_LEVEL:\n",
    "    for row in tqdm(X_train, position=0, leave=False): words.update(row.split())\n",
    "else:  \n",
    "    for row in tqdm(X_train, position=0, leave=False): words.update(list(row)) \n",
    "words.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No big surprises: the most frequent words are punctuation, end-of-sentence tokens, articles and prepositions.\n",
    "\n",
    "Now we drop all words with frequency less than 50 (what is specified under MIN_FREQ). Doing so makes our embedding matrix smaller. Decreasing MIN_FREQ will increase the accuracy of our model, at a higher memory and computation cost.\n",
    "\n",
    "We also add two special tokens: unknown (*unk*) and padding (*pad*). Unknown is any word not appearing in our list. These are any new words in our test or validation set, as well as the words we previously dropped due to low frequency. Padding will later be used to make all sequences of equal length. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "words = {k:v for k, v in tqdm(words.items(), leave = False) if v >= MIN_FREQ}\n",
    "words = sorted(words, key=words.get, reverse=True)\n",
    "words = ['<unk>','<pad>'] + words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "433309"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(words)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have more than 400k unique words. That's a lot for a regular NLP task, but considering there are 21 languages, it's not that surprising. The vocabulary size can be controlled by how many words we drop (MIN_FREQ).\n",
    "\n",
    "We need to create a mapping from words to integers (and back). I'll use the dictionaries below to do so. Note that unknown is mapped to 0, and padding is mapped to 1 (they are the first two elements by construction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = defaultdict(lambda: 0, {o:i for i,o in enumerate(words)})\n",
    "idx2word = defaultdict(lambda: '<unk>', {i:o for i,o in enumerate(words)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the first sentence of the training set converted into indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[678, 2, 162, 20319, 34964, 2, 248069, 76, 321362, 0, 2, 3612, 385755, 96462, 6984, 5866, 2350, 2, 66, 235447, 298230, 3]\n"
     ]
    }
   ],
   "source": [
    "if WORD_LEVEL:\n",
    "    print([word2idx[w] for w in X_train[0].split()])\n",
    "else:  \n",
    "    print([word2idx[w] for w in list(X_train[0])])  # For character-level models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply the above mapping to the whole training set. I also truncate long sentences to 32 words -- that should be more than enough to classify a language, and having more words would needlessly slow down computation time.\n",
    "\n",
    "Sentences shorter than 32 words are padded with the special (*pad*) character. This way, all example are of equal length, making subsequent computations easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2995643, 32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "X_train = utils.numericalize(X_train, word2idx, maxlen = SEQ_LEN, word_level = WORD_LEVEL)\n",
    "X_val = utils.numericalize(X_val, word2idx, maxlen = SEQ_LEN, word_level = WORD_LEVEL)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we can alway convert our indices back. Below is our first training example converted back to text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['To <punct> že Lisabonská smlouva <punct> dodává EU demokratičtější <unk> <punct> protože prý posiluje úlohu Evropského parlamentu <punct> je pouhá lež <eos>']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.de_numericalize(X_train[:1], idx2word, word_level = WORD_LEVEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also replace languages with contiguous integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang2idx = defaultdict(lambda: 0, {o:i for i,o in enumerate(LANGS)})\n",
    "idx2lang = defaultdict(lambda: '<unk>', {i:o for i,o in enumerate(LANGS)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cs' 'bg' 'sl' 'pt' 'da' 'es' 'fr' 'de' 'fr' 'lt']\n",
      "[14  8 15  4 12 16 11 19 11  3]\n"
     ]
    }
   ],
   "source": [
    "print(y_train[:10])  # Before\n",
    "y_train = np.array([lang2idx[x] for x in y_train])\n",
    "y_val = np.array([lang2idx[x] for x in y_val])\n",
    "print(y_train[:10])  # After"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(PATH_TMP/'numericalized.pickle', mode = 'wb') as f:\n",
    "#    dill.dump([words, vocab_size, word2idx, idx2word, X_train, X_val, y_train, y_val], f)\n",
    "#with open(PATH_TMP/'numericalized.pickle', mode = 'rb') as f:\n",
    "#    (words, vocab_size, word2idx, idx2word, X_train, X_val, y_train, y_val) = dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time after pre-processing : 3.264724044005076 mins\n"
     ]
    }
   ],
   "source": [
    "end = time.time()\n",
    "print(f'Time after pre-processing : {(end - start)/60} mins')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Dataloaders\n",
    "\n",
    "We take our training and validation data, convert them from numpy arrays to torch tensors, and put them in dataloaders. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.from_numpy(X_train).type(torch.int64)\n",
    "y_train = torch.from_numpy(y_train).type(torch.int64)\n",
    "X_val = torch.from_numpy(X_val).type(torch.int64)\n",
    "y_val = torch.from_numpy(y_val).type(torch.int64)\n",
    "\n",
    "train_dl = DataLoader(TensorDataset(X_train, y_train), batch_size=BS, shuffle = True)\n",
    "valid_dl = DataLoader(TensorDataset(X_val, y_val), batch_size=BS, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model\n",
    "\n",
    "Our model is a relatively simple recurrent network. We go through our sentence word by word, look it up in our embedding matrix, and feed it into our GRU. We apply a linear layer to the final output of our GRU, which gives our predictions. \n",
    "\n",
    "![Model Illustration](model_illustration.jpg)\n",
    "\n",
    "I also added dropout for regularization. I applied it at two points: first after our embedding lookup, and then before the final linear layer.\n",
    "\n",
    "Why not use something more complicated (say, a bidirectional LSTM)? It's simply not needed. The task is relatively simple, no need for an overkill.\n",
    "\n",
    "### On Hyperparameters\n",
    "\n",
    "The model is quite insensitive to changes in hyper-parameters. My experiments indicated that changing most modeling parameters by a factor of 0.2 - 5 doesn't change the validation accuracy much.\n",
    "\n",
    "The most surprising hyper-parameter is probably the embedding size. Usual NLP tasks have embedding dimensions of 200-700. However, our model doesn't need to learn a rich semantic representation for each word, only what is relevant to language prediction. Moreover, given our large vocabulary size we can easily run into memory errors if we choose too high of an embedding dimension. An embedding size of 50 suffices.\n",
    "\n",
    "I chose to use moderate amount dropout and a somewhat larger hidden layer than necessary. This choice just follows conventional deep learning advice: a regularized larger model usually performs better than an un-regularized smaller one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code below is a straightforward pytorch implementation of the concepts above\n",
    "class Lang_Detect(nn.Module):\n",
    "    def __init__(self, emb_sz = EMB_SZ, vocab_size = vocab_size,\n",
    "                 hidden_sz = HIDDEN_SZ, out_sz = len(LANGS), \n",
    "                 emb_drop = EMB_DROP, layer_drop = LAYER_DROP):\n",
    "        super().__init__()\n",
    "        self.hidden_sz = hidden_sz\n",
    "        self.emb = nn.Embedding(vocab_size, emb_sz)\n",
    "        self.emb_drop = nn.Dropout(emb_drop)\n",
    "        self.emb.weight.data.uniform_(-0.05, 0.05)  # Initialize embeddings\n",
    "        self.gru = nn.GRU(emb_sz, hidden_sz)\n",
    "        self.layer_drop = nn.Dropout(layer_drop)\n",
    "        self.lout = nn.Linear(hidden_sz, out_sz)        \n",
    "                \n",
    "    def forward(self, seq): \n",
    "        bs, _ = seq.shape\n",
    "        h =  torch.zeros(1, bs, self.hidden_sz).cuda()  # Initial empty hidden state\n",
    "        embedded = self.emb(seq).transpose(0, 1)\n",
    "        outputs, _ = self.gru(self.emb_drop(embedded), h)\n",
    "        output = self.lout(self.layer_drop(outputs[-1]))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Lang_Detect().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use standard cross-entropy as the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Model\n",
    "\n",
    "I create a special class that helps with fitting the model. It is model agnostic, and can help fit other pytorch models as well.\n",
    "\n",
    "First a helper function that calculates the loss for a batch, and updates parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_batch(xb, yb, model, loss_func, opt=None):\n",
    "    \n",
    "    '''Calculates the loss for a minibatch, and (if opt is given) updates parameters'''\n",
    "    # Taken from: https://github.com/fastai/fastai_v1/blob/master/dev_nb/001a_nn_basics.ipynb\n",
    "\n",
    "    loss = loss_func(model(xb.cuda()), yb.cuda())\n",
    "    if opt is not None:  # Update parameters\n",
    "        loss.backward(); opt.step(); opt.zero_grad()\n",
    "        \n",
    "    return loss.item(), len(xb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Learner class below has 3 methods to help the fitting process:\n",
    "- **lr_find**: to find the optimal learning rate,\n",
    "- **fit**: to fit the model,\n",
    "- **predict**: to predict on new data,\n",
    "- **plot_loss**: plot the training loss.\n",
    "\n",
    "The inspiration for this class is the [fast.ai](https://github.com/Gokkulnath/fastai-v0.7) library (version 0.7, now depreciated), which contains similar classes for fitting pytorch models.\n",
    "\n",
    "I use the Adam optimizer with default settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner(object):\n",
    "    \n",
    "    def __init__(self, model, loss_func, train_dl = None, valid_dl = None):\n",
    "        self.model = model\n",
    "        self.loss_func = loss_func\n",
    "        self.train_dl = train_dl\n",
    "        self.valid_dl = valid_dl\n",
    "        self.losses = []\n",
    "    \n",
    "    def lr_find(self, start = 1e-6, end = 1e3, exp_smooth_param = 0.95):\n",
    "        \n",
    "        '''Learning rate finder, detailed explanation below.'''\n",
    "        \n",
    "        old_state_dict = copy.deepcopy(self.model.state_dict())  # Save old parameters\n",
    "        self.model.train()\n",
    "        lr = start; lrs = []; losses = []; i = 0\n",
    "        for xb,yb in tqdm(self.train_dl, leave = False,\n",
    "                         position = 0):\n",
    "            opt = optim.Adam(self.model.parameters(), lr=lr)\n",
    "            loss, _ = loss_batch(xb, yb, self.model, self.loss_func, opt)\n",
    "            lrs.append(lr), losses.append(loss)\n",
    "            if (lr > end) or (i > 10 and loss > 3*np.mean(losses[:i])):\n",
    "                break  # Stop if losses shoot up higher than the average loss so far\n",
    "            lr *= 1.03; i += 1        \n",
    "        self.losses = losses  # Previous losses aren't relevant for plotting\n",
    "        self.plot_loss(x = lrs, xlog=True, exp_smooth_param = exp_smooth_param)\n",
    "        self.losses = []  # Reset list\n",
    "        # Note: we don't want to keep the parameters, as at the end of the algorithm\n",
    "        # very high learning rates are used, and the loss is high. Hence, we\n",
    "        # reset it\n",
    "        self.model.load_state_dict(old_state_dict)\n",
    "        \n",
    "    def plot_loss(self, x = None, xlog = False, exp_smooth_param = 0.95):\n",
    "        '''Plots training loss. Each datapoint is a mini-batch.'''\n",
    "        # Note: plots losses from all epochs (since the last lr_find call)\n",
    "        \n",
    "        # Use exponental smoothing with parameter exp_smooth_param\n",
    "        y_smooth = utils.exp_smooth(np.array(self.losses), exp_smooth_param)\n",
    "        f, ax = plt.subplots(figsize=(5, 5))\n",
    "        if xlog:  # Use log on both scales\n",
    "            ax.set(yscale = 'log', xscale = 'log')  \n",
    "        else: \n",
    "            ax.set(yscale = 'log')\n",
    "        if x is not None:\n",
    "            ax = plt.plot(x, y_smooth)\n",
    "        else:\n",
    "            ax = plt.plot(y_smooth)     \n",
    "            \n",
    "    def fit(self, lr, epochs):\n",
    "        \n",
    "        '''Fits model for specified number of epochs, with learning rate lr.'''\n",
    "        \n",
    "        opt = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "        \n",
    "            # Fit model to training data\n",
    "            self.model.train()  # Enables dropout\n",
    "            losses, nums = zip(*[loss_batch(xb, yb, self.model, self.loss_func, opt) \n",
    "                                 for xb,yb in tqdm(self.train_dl, leave = False, position = 0)])\n",
    "            train_loss = np.sum(np.multiply(losses,nums)) / np.sum(nums)\n",
    "            self.losses = self.losses+list(losses)\n",
    "            \n",
    "            # Apply model to validation data\n",
    "            if self.valid_dl != None:             \n",
    "                self.model.eval()  # Disables dropout\n",
    "                with torch.no_grad():  # No gradient calculations\n",
    "                    \n",
    "                    # Note: loss_batch doesn't update parameters if not supplied with\n",
    "                    # an optimizer. So this loop only calculates the loss.\n",
    "                    losses,nums = zip(*[loss_batch(xb, yb, self.model, self.loss_func)\n",
    "                                        for xb,yb in valid_dl])\n",
    "                    val_loss = np.sum(np.multiply(losses,nums)) / np.sum(nums)\n",
    "                    \n",
    "                    # Also compute validation accuracy\n",
    "                    val_preds = self.predict(self.valid_dl)\n",
    "                    y_val = self.valid_dl.dataset.tensors[1]\n",
    "                    acc = utils.accuracy(val_preds, y_val) \n",
    "                    # Note: in this case, refactoring code means we run the model through\n",
    "                    # the validation set twice. That's okay - the validation set is small.\n",
    "                    \n",
    "                print(f'Epoch {epoch}. Training loss: {train_loss}. ' +\n",
    "                      f'Validation loss: {val_loss}. Accuracy: {acc}')  \n",
    "            else:  \n",
    "                print(f'Epoch {epoch}. Training loss: {train_loss}.')\n",
    "                        \n",
    "    def predict(self, dl):\n",
    "        '''Gives predictions on supplied dataloader, concatenates results.'''\n",
    "        self.model.eval()  # No dropout\n",
    "        with torch.no_grad():  # No gradient calculations\n",
    "            res = [self.model(xb.cuda()).argmax(dim = -1).view(-1) for \n",
    "                   xb, _ in tqdm(dl, leave = False, position = 0)]\n",
    "        return torch.cat(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(model, loss_func, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use our untrained model to predict the validation set. As expected, it's about as good as chance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "text/plain": [
       "0.013978849966953073"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = learn.predict(valid_dl)\n",
    "utils.accuracy(preds, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to fit our model, we have to give it a learning rate. A learning rate that's too high will not converge on a minimum, a learning rate that's too low will take too long to find one.\n",
    "\n",
    "The lr_find method helps us pick a good learning rate: neither too high, nor too low. Originally, the idea was mentioned in [this](https://arxiv.org/abs/1506.01186) paper, a nice blog post explaining the concept is [here](https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html).\n",
    "\n",
    "We start fitting our neural net with a very low learning rate (1e-6). Then after each minibatch, we increase it a little (by 3%). We do it until we reach a rate that's too high (1e3), or until our loss sharply increases.\n",
    "\n",
    "What tends to happen: at low learning rates, our loss doesn't improve. At a certain point, it decreases steeply. However, moving past this optimal range, or loss shoots up as we are taking update steps too big.\n",
    "\n",
    "Here is our (smoothed) loss plotted against the learning rate. Note that we don't want to pick the minimum: at that point the learning rate is already too high. Instead we want to pick a point where we are improving at a steady-but-safe rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To err on the safe side, I choose a relatively low rate. Training this network is relatively fast anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Training loss: 0.07616978817877502. Validation loss: 0.023474243133570764. Accuracy: 0.9932253800396563\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit(LR, EPOCHS)\n",
    "#dill.dump(learn.model.state_dict(), open(PATH_TMP/'model0.pickle', mode = 'wb'))\n",
    "learn.plot_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are already achieving good validation accuracy. Our loss seems to stagnate after ~40k mini-batches, so training at this learning rate any more is pointless. Instead, let's decrease our learning rate by two orders of magnitude and fit just a little more to fine-tune our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5851 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Training loss: 0.017032668731622548. Validation loss: 0.0213000378424335. Accuracy: 0.9940185062789161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5851 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1. Training loss: 0.016206620863552384. Validation loss: 0.0209592488886738. Accuracy: 0.9940515532055518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2. Training loss: 0.015917065906295273. Validation loss: 0.020741176022178513. Accuracy: 0.9940185062789161\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit(LR/1e2, EPOCHS_FT)\n",
    "#dill.dump(learn.model.state_dict(), open(PATH_TMP/'model1.pickle', mode = 'wb'))\n",
    "learn.plot_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though it's not noticeable on the plot, our loss is actually still decreasing. We could train it a bit further, maybe even experiment with lower learning rates for further fine-tuning. However, our model is pretty good at this point - and early stopping helps with generalizability anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time after training : 11.832406417528789 mins\n"
     ]
    }
   ],
   "source": [
    "end = time.time()\n",
    "print(f'Time after training : {(end - start)/60} mins')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Test Set\n",
    "\n",
    "We start with mapping our test text to integers as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "tt_start = time.time()\n",
    "X_test = utils.numericalize(np.array(test['text']), word2idx, maxlen = SEQ_LEN, word_level = WORD_LEVEL)\n",
    "y_test = np.array([lang2idx[x] for x in test['label']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then convert our arrays to torch tensors and put them into a data-loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = torch.from_numpy(X_test).type(torch.int64)\n",
    "y_test = torch.from_numpy(y_test).type(torch.int64)\n",
    "\n",
    "test_dl = DataLoader(TensorDataset(X_test, y_test), batch_size=BS, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time for prediction : 1.2247247695922852 secs\n",
      "Test set accuracy: 0.9990397541770694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "preds = learn.predict(test_dl)\n",
    "test_time = test_time + time.time() - tt_start\n",
    "print(f'Total time for prediction : {(test_time)} secs')\n",
    "print(f'Test set accuracy: {utils.accuracy(preds, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our test set accuracy is great. Let's have a look at accuracy by language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of mispredicted: 20 out of 20828\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "label\n",
       "sk    0.995694\n",
       "cs    0.996000\n",
       "da    0.997000\n",
       "pl    0.997000\n",
       "et    0.998000\n",
       "sl    0.998000\n",
       "ro    0.998922\n",
       "hu    0.999000\n",
       "pt    1.000000\n",
       "nl    1.000000\n",
       "lv    1.000000\n",
       "lt    1.000000\n",
       "bg    1.000000\n",
       "fr    1.000000\n",
       "fi    1.000000\n",
       "es    1.000000\n",
       "en    1.000000\n",
       "el    1.000000\n",
       "de    1.000000\n",
       "it    1.000000\n",
       "sv    1.000000\n",
       "Name: correct, dtype: float64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['pred'] = [idx2lang[x] for x in utils.conv2np(preds)]\n",
    "test['correct'] = (test['pred'] == test['label'])*1\n",
    "inc_total = len(test.index)-sum(test['correct'])\n",
    "print(f\"Total number of mispredicted: {inc_total} out of {len(test.index)}\")\n",
    "test.groupby(by = 'label')['correct'].agg('mean').sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For almost all languages we achieve 100% accuracy. It's also not surprising that Czech and Slovak have the worst accuracy. \n",
    "\n",
    "Let's have a look at the examples we mis-predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_incorrect(n):\n",
    "    \n",
    "    '''Prints n incorrectly classified sentences.'''\n",
    "    \n",
    "    misclas = test[test['correct']==0]\n",
    "    ex = misclas.sample(min(n, len(misclas.index)), replace = False)\n",
    "    \n",
    "    for idx in ex.index:\n",
    "        print(f'class: {test.iloc[idx][\"label\"]}, ', end = \"\")\n",
    "        print(f'predicted: {test.iloc[idx][\"pred\"]}, ', end = \"\")\n",
    "        temp = utils.de_numericalize(X_test.numpy()[idx:(idx+1)], idx2word, word_level = WORD_LEVEL)\n",
    "        print(f'text: {temp}, ', end = \"\")\n",
    "        print('\\n')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class: hu, predicted: fi, text: ['<punct> <unk> Martin Schulz részéről <punct> <eos>'], \n",
      "\n",
      "class: et, predicted: el, text: ['<unk> finantsturgudel <eos>'], \n",
      "\n",
      "class: sl, predicted: sk, text: ['To je recept za katastrofo <eos>'], \n",
      "\n",
      "class: cs, predicted: lt, text: ['<unk> <unk> <unk> <unk> <eos>'], \n",
      "\n",
      "class: pl, predicted: it, text: ['poprawki <num> <punct> <num> <punct> <num> <punct> <num> <punct> <num> <punct> <num> <punct> <num> <punct> <num> <punct> <num> <punct> <num> <punct> <num> <punct> <num> <punct> <num> <punct> <num> <punct> <num> <punct> <num>'], \n",
      "\n",
      "class: cs, predicted: et, text: ['Věc <punct> <unk> projektu Galileo <eos>'], \n",
      "\n",
      "class: da, predicted: sv, text: ['<unk> vi <unk> som en potentiel energikilde <eos>'], \n",
      "\n",
      "class: sk, predicted: cs, text: ['Stredomorská strava <punct> rozprava <punct> <eos>'], \n",
      "\n",
      "class: pl, predicted: da, text: ['<unk> te to belgijski region Limburg <punct> <unk> region Limburg i region Aachen <eos>'], \n",
      "\n",
      "class: da, predicted: en, text: ['Their will is the law <punct> not only at home <punct> but as to the concerns of every nation <eos> <eos> They have swept away the very constitutions under which the <unk>'], \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_incorrect(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are only 8 wrong predictions.\n",
    "1. One of the six is a data error (the 'danish' text is assigned the wrong label).\n",
    "1. Five are short sentences with low-frequency words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time : 11.841763937473297 mins\n"
     ]
    }
   ],
   "source": [
    "end = time.time()\n",
    "print(f'Total time : {(end - start)/60} mins')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run-Time\n",
    "\n",
    "The whole model takes ~2 hours to train. Training was done on google cloud, with the following machine specifications:\n",
    "\n",
    "- 1 P100 GPU\n",
    "- 30 GB Ram\n",
    "- 8 CPU cores\n",
    "\n",
    "As a pre-embtible instance, such a machine costs ~$0.5 / hour (as of 2018 November). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try with own example text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These examples are in English, German and Hungarian respectively\n",
    "own_examples = [\"Let's see if this sentence gets classified correctly.\", \n",
    " 'Das können wir auch ausprobieren',\n",
    " 'Ezt a szöveget is megpróbáljuk lefordítani.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_own(ex_list, learn_obj = learn, langmapper = idx2lang, \n",
    "                seq_len = SEQ_LEN,\n",
    "                word2idx = word2idx, word_level = WORD_LEVEL):\n",
    "    '''Predict the language of a a custom list of example sentences. Whole pipeline.'''\n",
    "    y = torch.zeros(len(ex_list))                                     # 1. Dummy y for data loader\n",
    "    f = lambda x: utils.preprocess(x, word_level)\n",
    "    exampl = np.array(list(map(f, ex_list)))                          # 2. Preprocess, convert to numpy\n",
    "    idxs = utils.numericalize(exampl, word2idx, seq_len, word_level)  # 3. Numericalize\n",
    "    idxs = torch.from_numpy(idxs).type(torch.int64)                   # 4. Convert to torch tensor\n",
    "    dl = DataLoader(TensorDataset(idxs, y), batch_size=BS, shuffle = False)  # 6. Put in dataloader\n",
    "    res = learn_obj.predict(dl)                                       # 7. Predict language index\n",
    "    res = [langmapper[x] for x in utils.conv2np(res)]                 # 8. Map index to language\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "text/plain": [
       "['en', 'de', 'hu']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_own(own_examples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
